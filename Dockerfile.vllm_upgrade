# 1. Use the stable Blackwell-compatible production image
FROM nvidia/cuda:12.8.0-devel-ubuntu24.04

# --- BLACKWELL (SM120) & UNIFIED MEMORY SETTINGS ---
ENV TORCH_CUDA_ARCH_LIST="12.0"
ENV VLLM_TARGET_DEVICE="cuda"
ENV MAX_JOBS=2
ENV DEBIAN_FRONTEND=noninteractive

USER root

# 2. Install ARM64 build essentials
RUN apt-get update && apt-get install -y \
    python3.12 python3.12-dev python3-pip \
    cmake git ccache ninja-build wget gnupg2 \
    libgoogle-perftools-dev numactl \
    && rm -rf /var/lib/apt/lists/*

# 3. Setup Python 3.12
RUN ln -sf /usr/bin/python3.12 /usr/bin/python3 && \
    python3 -m pip install --upgrade pip setuptools wheel --break-system-packages --ignore-installed

# 4. Install Torch using the index that supports ARM64 + CUDA
# We remove the cu131 specific URL which is often x86_64 only
RUN pip install torch torchvision torchaudio --break-system-packages

# 5. Build vLLM 0.13.0
RUN git clone --depth 1 --branch v0.13.0 https://github.com/vllm-project/vllm.git /opt/vllm_src && \
    cd /opt/vllm_src && \
    pip install "numpy<2.0" "flashinfer-python" "fastsafetensors>=0.1.1" --break-system-packages && \
    pip install . --verbose --break-system-packages

# 6. VERSION GUARD
RUN python3 -c "import vllm; print(f'VERIFIED: vLLM {vllm.__version__} is ready.')"

ENTRYPOINT ["python3", "-m", "vllm.entrypoints.openai.api_server"]