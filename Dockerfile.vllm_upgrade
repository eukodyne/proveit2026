FROM nvcr.io/nvidia/vllm:25.12-py3

ENV TORCH_CUDA_ARCH_LIST="12.0"
ENV TRITON_PTXAS_PATH="/usr/local/cuda/bin/ptxas"
ENV VLLM_INSTALL_PUNICA_KERNELS=1
ENV MAX_JOBS=12
ENV UV_BREAK_SYSTEM_PACKAGES=1

USER root

# 1. Update Repo & Install CUDA Toolkit
RUN apt-get update && apt-get install -y wget gnupg2 && \
    wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2404/sbsa/cuda-keyring_1.1-1_all.deb && \
    dpkg -i cuda-keyring_1.1-1_all.deb && \
    apt-get update && apt-get install -y \
    cuda-toolkit-13-1 \
    python3.12-dev cmake git ccache ninja-build && \
    rm -rf /var/lib/apt/lists/*

# 2. CLEAR EXISTING vLLM
RUN pip uninstall -y vllm

# 3. Setup Build Tools
RUN pip install uv --break-system-packages && \
    uv pip install --system --break-system-packages "setuptools>=69.0" "setuptools_scm>=8.0" "wheel" "ninja"

# 4. Build vLLM 0.13.0
RUN git clone --depth 1 --branch v0.13.0 https://github.com/vllm-project/vllm.git /opt/vllm_src && \
    cd /opt/vllm_src && \
    uv pip install --system --break-system-packages \
    "ijson" "partial-json-parser" "py-cpuinfo" "xgrammar" "flashinfer-python" "fastsafetensors>=0.1.1" && \
    python3 setup.py install

# 5. Force binary link (Ensures 'vllm' command uses the new build)
RUN ln -sf /usr/local/bin/vllm /usr/bin/vllm

ENTRYPOINT ["python3", "-m", "vllm.entrypoints.openai.api_server"]