services:
  # 1. Milvus Vector DB (Standalone mode with embedded etcd/minio)
  milvus:
    image: milvusdb/milvus:v2.5.10-20250418-5a8c98a2-gpu-arm64
    container_name: p26-vector-db
    restart: always
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      - ETCD_USE_EMBED=true
      - ETCD_DATA_DIR=/var/lib/milvus/etcd
      - COMMON_STORAGETYPE=local
      - NVIDIA_VISIBLE_DEVICES=all
    volumes:
      - milvus_data:/var/lib/milvus
    ports:
      - "19530:19530"
      - "9091:9091"
    command: ["milvus", "run", "standalone"]
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # 2. LLM Server: GPT-OSS-20B via vLLM with MXFP4 quantization
  llm-server:
    build:
      context: .
      dockerfile: Dockerfile.gptoss20b
    container_name: p26-gpt-server
    restart: always
    ipc: host
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - VLLM_USE_V1=1
      - TORCH_CUDA_ARCH_LIST="12.0"
      - PYTHONWARNINGS=ignore::UserWarning:torchvision.io.image
    volumes:
      - /home/devpartner/.cache/huggingface:/root/.cache/huggingface
    ports:
      - "8000:8000"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    entrypoint: ["python3", "-m", "vllm.entrypoints.openai.api_server"]
    command:
      - --model=openai/gpt-oss-20b
      - --quantization=mxfp4
      - --max-model-len=32768
      - --gpu-memory-utilization=0.35
      - --trust-remote-code
      - --host=0.0.0.0
      - --port=8000

  # 3. RAG API
  rag-api:
    build: ./app
    image: rag-api:latest
    container_name: p26-rag-endpoint
    environment:
      - MILVUS_URL=http://milvus:19530
      - VLLM_URL=http://llm-server:8000/v1
    volumes:
      - ./app:/app
      - /home/devpartner/sop_documents:/documents
    ports:
      - "8080:8080"
    depends_on:
      - llm-server
      - milvus
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # 4. n8n Workflow Automation
  n8n:
    image: n8nio/n8n:latest
    container_name: p26-n8n
    restart: always
    dns:
      - 1.1.1.1
      - 8.8.8.8
    extra_hosts:
      - "host.docker.internal:host-gateway"
    environment:
      - N8N_HOST=0.0.0.0
      - N8N_PORT=5678
      - N8N_PROTOCOL=http
      - N8N_SECURE_COOKIE=false
      - N8N_EDITOR_BASE_URL=http://100.91.207.62:5678
      - WEBHOOK_URL=http://100.91.207.62:5678/
      - N8N_COMMUNITY_PACKAGES_ENABLED=true
    volumes:
      - n8n_data:/home/node/.n8n
    ports:
      - "5678:5678"
    depends_on:
      - rag-api
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

volumes:
  milvus_data:
  n8n_data:
