services:
  # 1. The LLM Server (vLLM) - original image: nvcr.io/nvidia/vllm:25.12-py3 - custom image: vllm-blackwell-native:0.13.0
  llm-server:
    image: nvcr.io/nvidia/vllm:25.12-py3
    container_name: gpt-server
    runtime: nvidia
    restart: always
    ipc: host
    ulimits:
      memlock: -1
      stack: 67108864
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - TIKTOKEN_CACHE_DIR=/app/tiktoken_cache
      - VLLM_USE_V1=1
      - VLLM_USE_FLASHINFER_MOE_MXFP4_MXFP8=0
      - VLLM_USE_FLASHINFER_MOE_NVFP4=1
      - VLLM_USE_TRTLLM_ATTENTION=1
      - TORCH_CUDA_ARCH_LIST="12.0"
      - PYTHONWARNINGS=ignore::UserWarning:torchvision.io.image
      - LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:$LD_LIBRARY_PATH
    volumes:
      - /home/devmaster/.cache/huggingface:/root/.cache/huggingface
      - /home/devmaster/tiktoken_cache:/app/tiktoken_cache
    ports:
      - "8000:8000"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"   # Max size of a single log file
        max-file: "3"     # Keep only the last 3 files
    command: >
      vllm serve 2imi9/gpt-oss-20b-NVFP4
      --max-model-len 131072
      --gpu-memory-utilization 0.70
      --trust-remote-code
      --quantization nvfp4

  # 2. Milvus Vector DB (Standalone mode with embedded etcd/minio)
  milvus:
    image: milvusdb/milvus:v2.5.10-20250418-5a8c98a2-gpu-arm64
    container_name: milvus-standalone
    runtime: nvidia
    restart: always
    environment:
      - ETCD_USE_EMBED=true
      - ETCD_DATA_DIR=/var/lib/milvus/etcd
      - COMMON_STORAGETYPE=local
    volumes:
      - milvus_data:/var/lib/milvus
    ports:
      - "19530:19530"
      - "9091:9091"
    command: ["milvus", "run", "standalone"] 
    logging:
      driver: "json-file"
      options:
        max-size: "10m"   # Max size of a single log file
        max-file: "3"     # Keep only the last 3 files

  # 3. Your RAG API
  rag-api:
    build:
      context: ./app
    container_name: rag-endpoint
    environment:
      - MILVUS_URL=http://milvus:19530
      - VLLM_URL=http://llm-server:8000/v1
    volumes:
      - ./app:/app
      - /home/devmaster/sop_documents:/documents
    ports:
      - "8080:8080"
    depends_on:
      - llm-server
      - milvus
    logging:
      driver: "json-file"
      options:
        max-size: "10m"   # Max size of a single log file
        max-file: "3"     # Keep only the last 3 files

volumes:
  milvus_data:
