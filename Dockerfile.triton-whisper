# Whisper Speech-to-Text via TensorRT-LLM (Open Source, No NIM)
# Optimized for Blackwell GPU (sm_120)
# Model: openai/whisper-large-v3-turbo
#
# Build process:
# 1. Pull NGC TensorRT-LLM container
# 2. Convert Whisper checkpoint to TRT-LLM format
# 3. Build optimized engines for encoder and decoder
# 4. Run FastAPI server with OpenAI-compatible endpoints

FROM nvcr.io/nvidia/tensorrt-llm/release:1.3.0rc0

ENV DEBIAN_FRONTEND=noninteractive
WORKDIR /workspace

# Install ffmpeg for audio decoding
RUN apt-get update && apt-get install -y --no-install-recommends ffmpeg \
    && rm -rf /var/lib/apt/lists/*

# Install additional dependencies for API server
RUN pip install --no-cache-dir \
    fastapi \
    uvicorn \
    python-multipart \
    librosa \
    soundfile \
    openai-whisper

# The TensorRT-LLM examples are already in the container at /app/tensorrt_llm/examples
# Navigate to whisper example
WORKDIR /app/tensorrt_llm/examples/models/core/whisper

# Install whisper-specific requirements
RUN pip install --no-cache-dir -r requirements.txt

# Copy API server and entrypoint scripts
COPY triton_whisper_server.py /workspace/server.py
COPY entrypoint-whisper.sh /workspace/entrypoint.sh
RUN chmod +x /workspace/entrypoint.sh

WORKDIR /workspace

EXPOSE 8001

# Entrypoint will convert model on first run (requires GPU)
ENTRYPOINT ["/workspace/entrypoint.sh"]
CMD ["python3", "server.py"]
