# Whisper Speech-to-Text via vLLM
# Model: RedHatAI/whisper-large-v3-FP8-dynamic
# Based on Dockerfile.vllm_upgrade
#
# WARNING: This vLLM-based Whisper has known bugs and produces inaccurate
# transcriptions. Use Dockerfile.triton-whisper (TensorRT-LLM) instead
# until vLLM encoder-decoder bugs are fixed.

FROM nvidia/cuda:13.1.0-devel-ubuntu24.04

ENV DEBIAN_FRONTEND=noninteractive
ENV PIP_BREAK_SYSTEM_PACKAGES=1

# Build parallelism settings (prevent OOM)
ARG BUILD_JOBS=8
ENV MAX_JOBS=${BUILD_JOBS}
ENV CMAKE_BUILD_PARALLEL_LEVEL=${BUILD_JOBS}

# PyTorch/CUDA settings for Blackwell SM120
ENV TORCH_CUDA_ARCH_LIST=12.1a
ENV TRITON_PTXAS_PATH=/usr/local/cuda/bin/ptxas

# uv package manager settings
ENV UV_SYSTEM_PYTHON=1
ENV UV_BREAK_SYSTEM_PACKAGES=1

# Install runtime dependencies including CUDA libs and audio processing
RUN apt update && apt upgrade -y \
    && apt install -y --allow-change-held-packages --no-install-recommends \
        python3 python3-pip python3-dev \
        git wget curl jq \
        libcudnn9-cuda-13 \
        libnccl-dev libnccl2 \
        ffmpeg libsndfile1 \
    && rm -rf /var/lib/apt/lists/* \
    && pip install uv

# Install vLLM nightly with auto PyTorch backend (handles CUDA deps automatically)
# Using nightly for CUDA 13 compatibility
RUN uv pip install -U vllm \
    --torch-backend=auto \
    --extra-index-url https://wheels.vllm.ai/nightly/cu130

# Install FlashInfer for attention optimization
RUN uv pip install flashinfer-python -U --no-deps --index-url https://flashinfer.ai/whl && \
    uv pip install flashinfer-cubin --index-url https://flashinfer.ai/whl && \
    uv pip install flashinfer-jit-cache --index-url https://flashinfer.ai/whl/cu130

# Install audio processing dependencies for Whisper
RUN uv pip install librosa soundfile openai-whisper

# Version verification
RUN python3 -c "import torch; print(f'PyTorch {torch.__version__}, CUDA: {torch.version.cuda}')" && \
    python3 -c "import vllm; print(f'vLLM {vllm.__version__}')"

# Expose API port
EXPOSE 8001

ENTRYPOINT ["python3", "-m", "vllm.entrypoints.openai.api_server"]
CMD ["--model=openai/whisper-large-v3-turbo", "--host=0.0.0.0", "--port=8001", "--gpu-memory-utilization=0.1"]
