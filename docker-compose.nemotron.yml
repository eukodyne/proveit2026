# LLM Server Override: Nemotron-3-Nano via llama.cpp
# Usage: docker compose -f docker-compose.yml -f docker-compose.nemotron.yml up -d
# Build: docker build -f Dockerfile.Nemotron -t nemotron-server:latest .

services:
  llm-server:
    image: nemotron-server:latest
    container_name: gpt-server
    restart: always
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - LLAMA_CTX_SIZE=${LLAMA_CTX_SIZE:-32768}
      - LLAMA_N_GPU_LAYERS=999
    volumes:
      - /home/devpartner/.cache/huggingface:/models/nemotron
    ports:
      - "8000:8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
